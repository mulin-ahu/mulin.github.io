---
permalink: /
title: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Lin Mu is an Associate Professor at the School of Computer Science and Technology, Anhui University. He received his Ph.D. degree in Computer Application Technology from the University of Science and Technology of China (USTC) in April 2021. His primary research interests include natural language processing, large language model inference, and parameter-efficient fine-tuning methods. In recent years, he has published over 20 papers in leading international conferences and journals, including ACL, WWW, EMNLP, and TBD. He also serves as a reviewer for several top-tier international conferences such as ACL, EMNLP, NAACL, SIGIR, and AAAI.

News
======
Jan 15, 2026 our paper “From Representation to Clusters: A Contrastive Learning Approach for Attributed Hypergraph Clustering” has been accepted by The Web Conference 2026.

Publications
======
- Li Ni, Shuaikang Zeng, **Lin Mu**(Corresponding author) and Longlong Lin: From Representation to Clusters: A Contrastive Learning Approach for Attributed Hypergraph Clustering. The Web Conference 2026
- **Lin Mu**, Xiaoyu Wang, Li Ni, Yang Li, Zhize Wu, Peiquan Jin, Yiwen Zhang: DenseLoRA: Dense Low-Rank Adaptation of Large Language Models. ACL 2025
- **Lin Mu**, Jun Shen, Li Ni, Lei Sang, Zhize Wu, Peiquan Jin, Yiwen Zhang: DICP: Deep In-Context Prompt for Event Causality Identification. EMNLP 2025
- **Lin Mu**, Yide Cheng, Jun Shen, Yiwen Zhang, Hong Zhong:NetPrompt: Neural Network Prompting Enhances Event Extraction in Large Language Models. IEEE Transactions on Big Data 2025
- Li Ni, Haowen Shen, **Lin Mu**, Yiwen Zhang, Wenjian Luo: ComGPT: Detecting Local Community Structure With Large Language Models. IEEE Transactions on Computational Social Systems 2025
- **Lin Mu**, Wenhao Zhang, Yiwen Zhang, Peiquan Jin: DDPrompt: Differential Diversity Prompting in Large Language Models. ACL 2024（CCF A）
- **Lin Mu**, Peiquan Jin, Yiwen Zhang, Hong Zhong, Jie Zhao: Synonym Recognition from Short Texts: A Self-Supervised Learning Approach. ESWA 2023
